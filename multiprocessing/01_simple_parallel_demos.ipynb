{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51350a1f-29d5-4d71-a187-e510e18c1c62",
   "metadata": {},
   "source": [
    "# Demonstrate parallel execution with multiprocessing.Pool in Python\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "Create a set of hashes from a large dictionary of words. A relatively slow hash algorithm is used for demonstration purposes. The example is taken from https://superfastpython.com/multiprocessing-pool-python/.\n",
    "\n",
    "## Preparation\n",
    "\n",
    "Download the data from https://raw.githubusercontent.com.\n",
    "In a terminal window type \n",
    "```wget https://raw.githubusercontent.com/SuperFastPython/DataSets/main/bin/1m_words.txt.zip```\n",
    "Then unzip the file with `unzip 1m_words.txt.zip`\n",
    "You can remove the `__MACOS/` folder afterwards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5dc0a0-a07d-4d7a-8767-1c6fd0ef0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that shall be executed in parallel\n",
    "# Note: there is a certain overhead for creating the pool and executing threads. \n",
    "# Therefore, the function should be \"reasonably complex\" to achieve any speedup through parallelisation\n",
    "from hashlib import sha512\n",
    "# hash one word using the SHA algorithm\n",
    "def hash_word(word):\n",
    "    # create the hash object\n",
    "    hash_object = sha512()\n",
    "    # convert the string to bytes\n",
    "    byte_data = word.encode('utf-8')\n",
    "    # hash the word\n",
    "    hash_object.update(byte_data)\n",
    "    # get the hex hash of the word\n",
    "    return hash_object.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89022954-bc74-463c-95df-dda07a7303a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e1119-4335-417d-adb3-d7e6e4db05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of available CPUs\n",
    "print(os.cpu_count())\n",
    "# set number of processes for multiprocessing pool\n",
    "processes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57a9c4-9950-4751-9eea-13809378856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1m_words.txt', 'r') as f:\n",
    "    words = f.readlines()\n",
    "print(f'Read {len(words)} words from file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e19c29b-22b7-472b-af95-3411049ad3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 5\n",
    "# let's do a serial hashing first\n",
    "known_words = {hash_word(w) for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548858c-7907-4af5-ac08-99fd0d6b9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 5\n",
    "# now we do the same with parallel processes\n",
    "with Pool(processes) as p:\n",
    "    known_words = set(p.map(hash_word, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae289fcd-3d72-4d88-a689-f1c1e4dce287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in my trial runs, I find a speedup of ~20% using 4 or 8 processes (1.03 versus 1.19 seconds). \n",
    "# The program gets slower when I increase the number of processes to 32 or more.\n",
    "# Note: a bug in IPython 7.4 prevents the result value being set correctly with %%timeit!\n",
    "# Therefore, we need to execute the hashing once more to see the result\n",
    "with Pool(processes) as p:\n",
    "    known_words = set(p.map(hash_word, words))\n",
    "print(list(known_words)[9000:9010])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8881ac-72d9-4af2-b456-5bb36cbc6dce",
   "metadata": {},
   "source": [
    "## Asynchronous execution\n",
    "\n",
    "Here, we will use asynchronous mapping so that the main process can continue during the function calls.\n",
    "Note that results from the parallel tasks may not be avialble before all tasks have been executed, so the main program should not rely on them.\n",
    "\n",
    "Pool.close and Pool.join can be used to signal the end of the parallel processing and wait for the results.\n",
    "\n",
    "Again, the following example is from https://superfastpython.com/multiprocessing-pool-python/ with some modifications to illustrate continuation of the main program and an error callback function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676688b5-a268-493b-a0a4-75e66c5ccf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of the map_async and forget usage pattern\n",
    "from time import sleep\n",
    "from random import random\n",
    " \n",
    "# task to execute in a new process\n",
    "def task(value):\n",
    "    if value == 42 or value == 13:\n",
    "        raise ValueError(f'{value} is an impossible value!')\n",
    "    # generate a random value\n",
    "    random_value = random()\n",
    "    # block for moment\n",
    "    sleep(random_value)\n",
    "    # prepare result\n",
    "    result = (value, random_value)\n",
    "    # report results\n",
    "    print(f'>task({result[0]}):{result[1]:.3f}', end='\\n', flush=True)\n",
    "\n",
    "# error callback function\n",
    "def galactic_error(value):\n",
    "    print(f'***{value}***')\n",
    "    \n",
    "# main task; run this while the parallel tasks are executing. Nothing fancy...\n",
    "def main_task():\n",
    "    sleep_time = 0.04\n",
    "    for i in range(100):\n",
    "        print(f'$main({i}):{sleep_time:.3f}', end='\\n')\n",
    "        sleep(sleep_time)\n",
    "        \n",
    "# protect the entry point\n",
    "if __name__ == '__main__':\n",
    "    # create the process pool\n",
    "    with Pool(processes=8) as pool:\n",
    "        # issue tasks to the process pool\n",
    "        _ = pool.map_async(task, range(60), chunksize=16, error_callback=galactic_error)\n",
    "        # continue execution of main program\n",
    "        main_task()\n",
    "        # close the pool\n",
    "        pool.close()\n",
    "        # wait for all tasks to complete\n",
    "        pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a1b95c-45a0-43af-9637-eb81b901f8f6",
   "metadata": {},
   "source": [
    "## Summary\r\n",
    "\r\n",
    "from https://superfastpython.com/multiprocessing-pool-python/\r\n",
    "\r\n",
    "Use multiprocessing.Pool when\r\n",
    "\r\n",
    "* Your tasks can be defined by a pure function that has no state or side effects.\r\n",
    "\r\n",
    "* Your task can fit within a single Python function, likely making it simple and easy to understand.\r\n",
    "\r\n",
    "* You need to perform the same task many times, e.g. homogeneous tasks.\r\n",
    "\r\n",
    "* You need to apply the same function to each object in a collection in a for-loop.\r\n",
    "\r\n",
    "Process pools work best when applying the same pure function on a set of different data (e.g. homogeneous tasks, heterogeneous data). This makes code easier to read and debug.\r\n",
    "\r\n",
    "Donot Use multiprwcessing.Pool When\r\n",
    "\r\n",
    "* You have a single task; consider using the Process class with the \"target\" argument.\r\n",
    "\r\n",
    "* You have long running tasks, such as monitoring or scheduling; consider extending the Process class.\r\n",
    "\r\n",
    "* Your task functions require state; consider extending the Process class.\r\n",
    "\r\n",
    "* Your tasks require coordination; consider using a Process and patterns like a Barrier or Semaphore.\r\n",
    "\r\n",
    "* Your tasks require synchronization; consider using a Process and Locks.\r\n",
    "\r\n",
    "* You require a process trigger on an event; consider using the Process class.\r\n",
    "\r\n",
    "The sweet spot for process pools is in dispatching many similar tasks, the results of which may be used later in the program. Tasks that don't fit neatly into this summary are probably not a good fit for process pools.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d28f064-66c7-4586-8991-f88bc465fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c2b24-d2f3-4d6e-b219-bf6181777b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
